# üéì PhantomLink: Complete Beginner's Guide

*A comprehensive, educational guide to understanding brain-computer interfaces and neural data streaming*

---

## üìö Table of Contents

1. [What is PhantomLink?](#what-is-phantomlink)
2. [Brain-Computer Interfaces 101](#brain-computer-interfaces-101)
3. [Understanding Neural Channels](#understanding-neural-channels)
4. [Understanding Trials](#understanding-trials)
5. [Understanding Targets](#understanding-targets)
6. [Understanding Packets](#understanding-packets)
7. [Understanding Kinematics](#understanding-kinematics)
8. [Understanding Neural Manifolds](#understanding-neural-manifolds)
9. [Closed-Loop BCIs](#closed-loop-bcis)
10. [How It All Fits Together](#how-it-all-fits-together)
11. [Real-World Applications](#real-world-applications)
12. [Technical Deep Dive](#technical-deep-dive)
13. [Learning Resources](#learning-resources)

---

## What is PhantomLink?

### The Simple Explanation

**PhantomLink is like a "practice server" for brain-computer interfaces.**

Imagine you're building an app that needs to process emails, but you don't want to use real user emails during development. You'd use a service like [Mailtrap](https://mailtrap.io/) or [Ethereal](https://ethereal.email/) - they let you send and receive fake emails for testing.

**PhantomLink does the same thing, but for brain signals!**

Instead of connecting to someone's actual brain implant, developers can use PhantomLink to stream pre-recorded brain activity. This lets them:
- Build and test brain-reading algorithms safely
- Train AI models without needing live subjects
- Develop BCI applications faster and cheaper
- Learn how brain signals work without specialized equipment

### What PhantomLink Does

```
Real BCI Setup:
Brain ‚Üí Implant ‚Üí Computer ‚Üí Your Decoder ‚Üí Action

PhantomLink Setup:
Recorded Brain Data ‚Üí PhantomLink Server ‚Üí Your Decoder ‚Üí Action
```

PhantomLink replays **real neural recordings** at exactly 40 times per second (40Hz), just like they were recorded during an actual experiment. This gives developers realistic data to work with.

---

## Brain-Computer Interfaces 101

### What is a Brain-Computer Interface (BCI)?

A **BCI** is a direct connection between your brain and a computer. It reads brain activity and translates it into commands.

**Real-World Example:**
- Someone with paralysis has a chip implanted in their motor cortex (the part of the brain that controls movement)
- When they *think* about moving their hand, neurons in their motor cortex fire
- The chip records these neural signals
- A computer translates those signals into cursor movements on a screen
- The person can control a computer cursor just by thinking!

### Famous BCI Examples

1. **Neuralink** (Elon Musk's company) - Developing high-bandwidth brain implants
2. **BrainGate** - Helping paralyzed individuals control robotic arms
3. **Synchron** - Brain implant that doesn't require open brain surgery
4. **Facebook/Meta** - Working on non-invasive BCIs

### The Challenge

Building BCIs is hard because:
- **Ethics**: You can't just experiment on people's brains
- **Cost**: Brain implants and experiments are extremely expensive
- **Time**: It takes years to collect enough data
- **Access**: Very few research teams have access to live neural recordings

**This is where PhantomLink helps!** It democratizes BCI development by providing real brain data that anyone can use.

---

## Understanding Neural Channels

### The Biology

Your brain has about **86 billion neurons** (brain cells). When you think, move, or sense something, these neurons communicate by sending electrical signals called "action potentials" or "spikes."

### What is a Neural Channel?

A **neural channel** is like a **tiny microphone** placed in your brain to listen to these electrical signals.

**Analogy: Concert Hall**
```
üé∏ Guitarist playing     = One neuron firing
üé§ Microphone           = One neural channel (electrode)
üéôÔ∏è 142 microphones      = 142 neural channels
üéµ Recording of concert  = Dataset we're using
```

### In the MC_Maze Dataset

- **142 neural channels** = 142 tiny electrodes implanted in the motor cortex
- Each electrode is about the width of a human hair
- They're arranged in an array (like a tiny grid) in the brain
- Each records activity from several neurons nearby

### What Does a Channel Record?

Every 25 milliseconds (40 times per second), each channel counts:
- **How many neurons fired** near that electrode
- This is called a "spike count"

Example:
```
Channel 1: 3 spikes   (3 neurons fired nearby)
Channel 2: 0 spikes   (no activity)
Channel 3: 7 spikes   (very active!)
...
Channel 142: 2 spikes
```

### Why Multiple Channels?

Different neurons control different aspects of movement:
- Some neurons fire when you move **left**
- Some fire when you move **right**
- Some fire based on **speed**
- Some fire based on **force**

By recording from 142 channels simultaneously, we capture a rich "picture" of what the brain is planning to do.

**More channels = Better predictions = More accurate BCI**

---

## Understanding Trials

### What is a Trial?

A **trial** is **one complete attempt** at a task - from start to finish.

### Sports Analogy

Think of basketball free throws:
```
Trial 1: üèÄ ‚Üí üéØ (Made it! ‚úÖ)
Trial 2: üèÄ ‚Üí üéØ (Made it! ‚úÖ)
Trial 3: üèÄ ‚Üí ‚ùå (Missed üí•)
Trial 4: üèÄ ‚Üí üéØ (Made it! ‚úÖ)
```

Each throw is one trial. In our dataset, each trial is one reach toward a target.

### Trial Structure

```
üö¶ START (t=0s)
   ‚Üì
   Subject sees which target to reach for
   ‚Üì
üß† PLANNING
   ‚Üì
   Brain activity increases (neurons start firing)
   ‚Üì
üèÉ MOVEMENT
   ‚Üì
   Hand/cursor moves toward target
   ‚Üì
üéØ TARGET REACHED (t=~3s)
   ‚Üì
‚úÖ END
```

### In the MC_Maze Dataset

- **100 trials total**
- Average duration: **2.88 seconds** per trial
- **100% success rate** (all reaches were successful)
- Trials are back-to-back (one ends, next begins)

### Why Trials Matter

**1. Structure**
Trials break up continuous recording into meaningful chunks. Instead of "5 minutes of random brain activity," you have "100 organized attempts."

**2. Labels**
Each trial has a label (which target was reached). This is crucial for training AI:
```
Trial 5: Target 0 ‚Üí Brain signals ‚Üí Used to teach AI "this is what Target 0 looks like"
Trial 6: Target 1 ‚Üí Different signals ‚Üí "this is what Target 1 looks like"
```

**3. Variety**
Different trials = different movements = more diverse training data

**4. Analysis**
You can analyze:
- Success rates
- Reaction times
- Learning curves (do they get faster?)
- Neural patterns per target

---

## Understanding Targets

### What is a Target?

A **target** is a **physical location** the subject is trying to reach.

### The Experiment Setup

Imagine a screen with dots arranged in a pattern:

```
        üéØ Target 2
       (top center)
           ‚Üë
           |
üéØ Target 0 ‚Üê üè† ‚Üí üéØ Target 1
 (center)  Start  (right)
```

In the MC_Maze dataset:
- **3 targets** (Target 0, Target 1, Target 2)
- Positioned at different locations in 2D space
- Subject starts at center, reaches outward

### Center-Out Reaching Task

This is a **classic neuroscience experiment**:

1. Subject rests at center position
2. One target lights up (e.g., Target 1)
3. Subject moves toward that target
4. Target is reached ‚Üí Trial complete!
5. Return to center, repeat

**Why this design?**
- Simple enough to repeat many times
- Complex enough to require planning
- Different directions activate different neurons
- Standardized task used across research labs

### Target Distribution

In our dataset:
```
Target 0: ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà 77 trials (most common)
Target 1: ‚ñà‚ñà‚ñà‚ñà 11 trials
Target 2: ‚ñà‚ñà‚ñà‚ñà 12 trials
```

Target 0 is likely the "home" or most frequently used position.

### Why Targets Matter for BCIs

**Directional Tuning:**
Neurons in motor cortex have "preferred directions." Some neurons fire more when you move:
- ‚¨ÜÔ∏è Up
- ‚û°Ô∏è Right  
- ‚¨áÔ∏è Down
- ‚¨ÖÔ∏è Left

By recording trials to different targets, we can:
1. Figure out each neuron's preferred direction
2. Build a decoder that understands "these neurons firing = moving right"
3. Predict intended movement direction from brain activity

**This is the core of how BCIs work!**

---

## Understanding Packets

### What is a Packet?

A **packet** is **one snapshot of brain activity** at a single moment in time.

### The Movie Analogy

Think of a movie:
```
üé¨ Movie = The entire recording (293.7 seconds)
üéûÔ∏è Frame = One packet (1/40th of a second)
üìπ Frame rate = 40 FPS (frames per second)
```

Just like a movie is made of many frames played back quickly, the neural recording is made of many packets streamed at 40Hz.

### What's in a Packet?

Each packet contains a **complete snapshot** of everything happening at that moment:

```json
{
  "timestamp": 1.525,           // Time: 1.525 seconds
  "sequence_number": 61,         // This is packet #61
  
  "spikes": {
    "channel_ids": [0, 1, 2, ..., 141],
    "spike_counts": [3, 0, 7, 2, ...]  // Activity on all 142 channels
  },
  
  "kinematics": {
    "vx": -8.383,                // Hand moving left at 8.383 units/s
    "vy": -4.490,                // Hand moving down at 4.490 units/s
    "x": -112.5,                 // Current X position
    "y": -87.3                   // Current Y position
  },
  
  "intention": {
    "target_id": 0,              // Reaching for Target 0
    "target_x": -120.0,          // Target is at X=-120
    "target_y": -90.0            // Target is at Y=-90
  },
  
  "trial_id": 10                 // This is from Trial 10
}
```

### The 40Hz Streaming Rate

**Why 40Hz?**
- **Fast enough** to capture rapid neural changes
- **Slow enough** to process in real-time
- **Standard rate** used in many BCI studies
- **25ms intervals** = 0.025 seconds between packets

**Timing is critical!** BCIs need to respond quickly:
```
Too slow (10Hz):   Laggy, cursor jerks
Just right (40Hz): Smooth, natural control
Too fast (1000Hz): Unnecessary, hard to process
```

### Packet Sequence

Packets are numbered continuously:
```
Packet 0    ‚Üí Trial 0 starts
Packet 1    ‚Üí 
Packet 2    ‚Üí
...
Packet 136  ‚Üí Trial 0 ends
Packet 137  ‚Üí Trial 1 starts
Packet 138  ‚Üí
...
Packet 11745 ‚Üí Last packet, Trial 99 ends
```

### Why Packets Matter

**1. Real-Time Processing**
BCIs must work in real-time. Packets let you process data as it arrives:
```
Packet arrives ‚Üí Run decoder ‚Üí Predict movement ‚Üí Update cursor
(All within 25ms!)
```

**2. Streaming Architecture**
Modern BCIs stream data over networks (WebSockets). Packets are the unit of transmission.

**3. Synchronization**
The `sequence_number` ensures packets aren't lost or out-of-order.

**4. Ground Truth**
Each packet includes what the subject *actually did* (kinematics) vs. what they *intended* (target). This lets you validate your decoder.

---

## How It All Fits Together

### The Complete Hierarchy

```
üì¶ Dataset: MC_Maze
‚îÇ
‚îú‚îÄ‚îÄ üß† Neural Channels: 142
‚îÇ   ‚îî‚îÄ‚îÄ Each channel records nearby neuron activity
‚îÇ
‚îú‚îÄ‚îÄ üéØ Targets: 3
‚îÇ   ‚îú‚îÄ‚îÄ Target 0 (center, most common)
‚îÇ   ‚îú‚îÄ‚îÄ Target 1 (right)
‚îÇ   ‚îî‚îÄ‚îÄ Target 2 (top)
‚îÇ
‚îú‚îÄ‚îÄ üìã Trials: 100
‚îÇ   ‚îú‚îÄ‚îÄ Trial 0: Reach to Target 0 (3.42s, 137 packets)
‚îÇ   ‚îú‚îÄ‚îÄ Trial 1: Reach to Target 0 (2.13s, 85 packets)
‚îÇ   ‚îú‚îÄ‚îÄ Trial 2: Reach to Target 0 (2.84s, 114 packets)
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ üì° Packets: 11,746 total
    ‚îú‚îÄ‚îÄ Packet 0: timestamp=0.000s, trial=0, target=0
    ‚îú‚îÄ‚îÄ Packet 1: timestamp=0.025s, trial=0, target=0
    ‚îî‚îÄ‚îÄ ...
```

### Timeline Visualization

```
Time:     0s    1s    2s    3s    4s    5s    6s    7s    8s
          |-----|-----|-----|-----|-----|-----|-----|-----|
Trial 0:  [==========Target 0==========]
Packets:  ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (137 packets)
          
Trial 1:                               [====Target 0====]
Packets:                               ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (85 packets)

Trial 2:                                                [=====Target 0======]
Packets:                                                ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà (114 packets)
```

Each `‚ñà` represents one packet (25ms of data).

---

## Understanding Kinematics

### What are Kinematics?

**Kinematics** is the study of **motion** without considering the forces that cause it. In BCI research, kinematics describe how the hand, cursor, or limb is moving.

Think of kinematics as the "GPS data" of movement - it tells you where you are, how fast you're moving, and in what direction.

### The Core Variables

In the MC_Maze dataset, kinematics include **4 key measurements** for each packet:

```python
{
  "x": -112.5,     # X position (left-right)
  "y": -87.3,      # Y position (up-down)
  "vx": -8.383,    # X velocity (speed left-right)
  "vy": -4.490     # Y velocity (speed up-down)
}
```

### Position (x, y)

**What it is:** The current location of the cursor/hand in 2D space.

**Units:** Typically millimeters or screen coordinates

**Example:**
```
      +Y (up)
       ‚Üë
       |
  -X ‚Üê ‚Ä¢ ‚Üí +X  
       |
       ‚Üì
      -Y (down)

Position (0, 0) = Center
Position (120, 0) = 120 units to the right
Position (0, -90) = 90 units down
```

**Why it matters:**
- Tells you where the hand/cursor currently is
- Essential for determining when target is reached
- Used to calculate movement trajectories

### Velocity (vx, vy)

**What it is:** How fast the cursor/hand is moving in each direction.

**Units:** Units per second (e.g., mm/s)

**Example:**
```
vx = 50, vy = 0    ‚Üí Moving right at 50 units/s
vx = 0, vy = -30   ‚Üí Moving down at 30 units/s
vx = 20, vy = 20   ‚Üí Moving diagonally (up-right)
vx = 0, vy = 0     ‚Üí Not moving (stationary)
```

**Speed vs. Velocity:**
- **Velocity** includes direction (vx, vy)
- **Speed** is just the magnitude: `speed = ‚àö(vx¬≤ + vy¬≤)`

**Example calculation:**
```python
vx = -8.383
vy = -4.490
speed = sqrt((-8.383)¬≤ + (-4.490)¬≤) = 9.49 units/s
```

**Why it matters:**
- Neural activity correlates strongly with movement speed
- Different neurons fire at different speeds
- Critical for predicting intended movement
- Used in velocity-based decoders (predicting speed rather than position)

### Understanding Movement Phases

During a reach trial, kinematics reveal distinct phases:

```
Phase 1: REST
‚îú‚îÄ Position: (0, 0) - at center
‚îú‚îÄ Velocity: (0, 0) - not moving
‚îî‚îÄ Neural: Low baseline activity

Phase 2: PLANNING  
‚îú‚îÄ Position: (0, 0) - still at center
‚îú‚îÄ Velocity: (0, 0) - not moving yet
‚îî‚îÄ Neural: Activity increasing (preparing)

Phase 3: MOVEMENT (acceleration)
‚îú‚îÄ Position: (5, -3) ‚Üí (25, -15) ‚Üí (50, -30)
‚îú‚îÄ Velocity: (10, -6) ‚Üí (40, -24) ‚Üí (60, -36)
‚îî‚îÄ Neural: High activity, peak firing rates

Phase 4: MOVEMENT (deceleration)
‚îú‚îÄ Position: (100, -80) ‚Üí (115, -88)
‚îú‚îÄ Velocity: (30, -18) ‚Üí (10, -6) ‚Üí (2, -1)
‚îî‚îÄ Neural: Activity decreasing

Phase 5: TARGET HOLD
‚îú‚îÄ Position: (120, -90) - at target
‚îú‚îÄ Velocity: (0, 0) - stopped
‚îî‚îÄ Neural: Return to baseline
```

### Kinematic Features for Decoding

**Why track kinematics?**

**1. Ground Truth Labels**
Kinematics tell you what actually happened, which is crucial for training decoders:
```python
# Training data
Neural activity: [spike_counts] ‚Üí Model ‚Üí Predicted velocity: (vx, vy)
Actual velocity from kinematics: (vx=10, vy=5)
Error = Predicted - Actual
```

**2. Multiple Decoding Strategies**

You can decode different things:

```
Strategy A: Position Decoder
Neural activity ‚Üí Predict (x, y)
Best for: Cursor control, reaching tasks

Strategy B: Velocity Decoder  
Neural activity ‚Üí Predict (vx, vy)
Best for: Smooth continuous movement, prosthetics

Strategy C: Direction Decoder
Neural activity ‚Üí Predict movement angle
Best for: Simple directional control
```

**3. Feature Engineering**

Derived kinematic features improve decoding:

```python
# Speed (magnitude)
speed = sqrt(vx¬≤ + vy¬≤)

# Direction (angle)  
angle = arctan2(vy, vx)

# Distance to target
dist = sqrt((target_x - x)¬≤ + (target_y - y)¬≤)

# Movement curvature, acceleration, jerk...
```

### Visualization: Position vs. Velocity

**Position trajectory for Trial 5:**
```
     Start (0,0)
        ‚Ä¢
        |\
        | \
        |  \
        |   \
        |    \
        |     \
        |      \
        |       ‚Ä¢
        |      Target (120, -90)
       
Clean path from center to target
```

**Velocity profile for same trial:**
```
Speed (units/s)
   60 |     ___
      |    /   \
   40 |   /     \
      |  /       \
   20 | /         \___
      |/              
    0 |________________
      0    1    2    3s
      
Bell-shaped: Accelerate ‚Üí Peak ‚Üí Decelerate
```

### Neural Correlates of Kinematics

**The magic of motor cortex:**

Neurons in M1 (motor cortex) encode kinematics in their firing rates:

```
High velocity ‚Üí High firing rates
Low velocity  ‚Üí Low firing rates
Rightward     ‚Üí Some neurons fire more
Leftward      ‚Üí Different neurons fire more
```

**Cosine Tuning Model:**

Many motor cortex neurons follow this pattern:

```
firing_rate = baseline + Œ± * cos(Œ∏ - Œ∏_preferred) * speed

Where:
- Œ∏ = movement direction
- Œ∏_preferred = neuron's preferred direction
- Œ± = tuning strength
- speed = movement speed
```

**Example:**
```
Neuron 42 prefers rightward movement (Œ∏_preferred = 0¬∞)

Moving right (Œ∏=0¬∞):   High firing rate ‚úì
Moving left (Œ∏=180¬∞):  Low firing rate ‚úó
Moving up (Œ∏=90¬∞):     Medium firing rate ~
```

This is why we can decode movement from neural activity!

---

## Understanding Neural Manifolds

### What is a Neural Manifold?

A **neural manifold** is a **low-dimensional space** that captures the essential patterns of high-dimensional neural activity.

**The Challenge:**
- You record from 142 neural channels
- Each channel can have 0-20+ spikes
- That's a **142-dimensional space** - impossible to visualize or intuitively understand!

**The Solution:**
- Neural activity doesn't use all 142 dimensions randomly
- Movement-related activity lies on a **low-dimensional manifold**
- Think of it as a "road" through high-dimensional space

### The Map Analogy

Imagine tracking a road trip:

```
High-dimensional view:
- 142 measurements per second
  ‚Ä¢ Engine temperature
  ‚Ä¢ Tire pressure (√ó4)
  ‚Ä¢ Fuel level
  ‚Ä¢ Speed
  ‚Ä¢ GPS coordinates (x, y, z)
  ‚Ä¢ Acceleration (x, y, z)
  ‚Ä¢ ... [142 total measurements]

Low-dimensional manifold:
- The actual path you drove can be drawn on a 2D map
- All 142 measurements are constrained by the fact you're following roads
- The "manifold" is the road network
```

**Key insight:** Even though you track 142 things, the **actual behavior** (following roads) exists in a much simpler, lower-dimensional space.

### Neural Manifolds in Motor Control

**High-dimensional neural space:**
```
Channel 0: 3 spikes
Channel 1: 0 spikes  
Channel 2: 7 spikes
...
Channel 141: 2 spikes

‚Üí Point in 142-dimensional space
```

**Low-dimensional manifold:**
```
Instead of 142 random dimensions, movement-related activity follows patterns:

Dimension 1: "Rightward vs. Leftward"  
Dimension 2: "Upward vs. Downward"
Dimension 3: "Fast vs. Slow"
Dimension 4: "Reaching vs. Holding"

‚Üí Point in ~4-10 dimensional space
```

### Why Manifolds Matter for BCIs

**1. Dimensionality Reduction**

You can "compress" 142 channels into just a few meaningful dimensions:

```python
# Raw neural data
spikes = [3, 0, 7, 2, 1, ..., 2]  # 142 values

# Apply dimensionality reduction (e.g., PCA, UMAP)
latent = dimensionality_reduction(spikes)  
# latent = [0.3, -0.7, 1.2]  # Just 3 values!

# Decode from latent space
velocity = decoder(latent)
```

**Benefits:**
- Faster computation (3 values vs. 142)
- Less overfitting (fewer parameters to learn)
- Better generalization
- More interpretable (each dimension has meaning)

**2. Robustness**

Manifolds capture the **structure** of neural activity:

```
Problem: Electrode 42 stops working
Solution: The manifold is still mostly intact

Other electrodes still capture the same movement patterns
Decoder can often adapt automatically
```

**3. Understanding Neural Coding**

Manifolds reveal **how the brain organizes movement**:

```
Discovery: Reaching in different directions creates a circular pattern in manifold space

      Up
       ‚Üë
  Left ‚Ä¢ Right  ‚Üê Manifold structure
       ‚Üì
      Down

Implication: Brain uses a "rotational" code for direction
```

### Common Manifold Techniques

**1. PCA (Principal Component Analysis)**

**What it does:** Finds the directions of maximum variance in neural data

```python
from sklearn.decomposition import PCA

# 142-dimensional neural data
neural_data = spikes_matrix  # Shape: (n_samples, 142)

# Reduce to 3 dimensions
pca = PCA(n_components=3)
latent = pca.fit_transform(neural_data)  # Shape: (n_samples, 3)

# Now you can visualize in 3D!
```

**Result:**
```
PC1 (50% variance): Captures overall activity level (moving vs. resting)
PC2 (25% variance): Captures left-right direction
PC3 (15% variance): Captures up-down direction
```

**2. UMAP (Uniform Manifold Approximation and Projection)**

**What it does:** Preserves local structure, great for visualization

```python
import umap

reducer = umap.UMAP(n_components=2)
latent_2d = reducer.fit_transform(neural_data)

# Plot colored by target
plt.scatter(latent_2d[:, 0], latent_2d[:, 1], c=target_ids)
```

**Result:**
```
     ‚Ä¢        ‚Ä¢ ‚Üê Target 2 trials
    ‚Ä¢‚Ä¢       ‚Ä¢‚Ä¢
   
  ‚Ä¢‚Ä¢‚Ä¢‚Ä¢     Target 0 trials
 ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Ä¢
  ‚Ä¢‚Ä¢‚Ä¢‚Ä¢
  
     ‚Ä¢‚Ä¢‚Ä¢‚Ä¢‚Üê Target 1 trials
      ‚Ä¢‚Ä¢
```

Trials to the same target cluster together in manifold space!

**3. Neural Latents (What PhantomLink Uses)**

The **Neural Latents Benchmark** provides datasets specifically for studying manifolds:

```
Goal: Learn low-dimensional latent representations that:
1. Capture neural dynamics
2. Predict behavior (kinematics)
3. Generalize across trials
```

**Common architectures:**
- **Autoencoders:** Learn latent codes that reconstruct neural activity
- **VAEs (Variational Autoencoders):** Add probabilistic structure
- **RNNs/LSTMs:** Capture temporal dynamics
- **LFADS:** State-of-the-art latent factor analysis

### Manifold Analysis Example

**Analyzing the MC_Maze data:**

```python
# Step 1: Collect all neural activity
all_spikes = []
all_targets = []

for trial in range(100):
    spikes = get_trial_spikes(trial)  # Shape: (n_timepoints, 142)
    all_spikes.append(spikes)
    all_targets.extend([trial.target_id] * len(spikes))

X = np.vstack(all_spikes)  # Shape: (11746, 142)

# Step 2: Reduce to 2D manifold
from umap import UMAP
reducer = UMAP(n_components=2)
latent = reducer.fit_transform(X)  # Shape: (11746, 2)

# Step 3: Visualize
plt.scatter(latent[:, 0], latent[:, 1], 
           c=all_targets, cmap='viridis', alpha=0.5)
plt.title('Neural Manifold: MC_Maze')
plt.xlabel('Latent Dimension 1')
plt.ylabel('Latent Dimension 2')
```

**What you'll see:**
- Clear separation between targets
- Smooth trajectories through manifold space
- Shared structure (all movements start from similar place)
- Target-specific "reaches" in different directions

### Advanced: Dynamical Systems View

**Neural manifolds have dynamics:**

```
Think of neural activity as a ball rolling on a hilly landscape:

Current state (ball position) ‚Üí Neural activity now
Dynamics (gravity, slopes)    ‚Üí How activity evolves
Attractor (valley)            ‚Üí Stable movement pattern
```

**Example:**
```
Start: Resting state (center valley)
Cue:   Push the ball toward a target valley
Move:  Ball rolls along manifold (trajectory)
Hold:  Ball settles in target valley (attractor)
```

**Why this matters:**
- Predicting future states (where will the movement go?)
- Understanding stability (why are some movements more reliable?)
- Designing better decoders (that respect neural dynamics)

### Practical Takeaways

**For BCI Decoders:**

‚úÖ **Use manifold-aware methods:**
```python
# Instead of raw spikes
decoder.fit(raw_spikes, kinematics)  # 142 dimensions

# Use latent representation
latents = manifold_reducer.transform(raw_spikes)  # 10 dimensions
decoder.fit(latents, kinematics)  # Faster, more robust!
```

‚úÖ **Visualize your data:**
```python
# Always plot neural manifolds to understand your data
# Look for structure, outliers, target separation
```

‚úÖ **Choose appropriate dimensionality:**
```python
# Too few dimensions: Loss of information
n_components = 2  # Probably too simple

# Too many dimensions: Overfitting, noise
n_components = 100  # Probably too complex

# Sweet spot for motor cortex
n_components = 8-15  # Usually works well
```

---

## Closed-Loop BCIs

### What is Closed-Loop?

A **closed-loop BCI** is a system where the user receives **continuous feedback** and can adapt their brain activity in real-time based on that feedback.

### Open-Loop vs. Closed-Loop

**Open-Loop (PhantomLink):**
```
Brain Activity ‚Üí Decoder ‚Üí Prediction
                              ‚Üì
                          (no feedback to brain)
```

The brain doesn't know what the decoder is doing. This is good for:
- Offline analysis
- Algorithm development
- Training initial decoders

**Closed-Loop (Real BCI):**
```
Brain Activity ‚Üí Decoder ‚Üí Prediction
      ‚Üë                         ‚Üì
      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Feedback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          (visual, haptic)
```

The brain sees the result and adapts. This is essential for:
- Actual BCI control
- Learning to use the interface
- Real-world applications

### The Feedback Loop

**Complete cycle (repeated every 25-40ms):**

```
1. NEURAL RECORDING
   ‚îú‚îÄ Electrodes capture brain activity
   ‚îî‚îÄ Spikes counted and transmitted

2. DECODING
   ‚îú‚îÄ Computer receives neural data
   ‚îú‚îÄ Runs decoder algorithm
   ‚îî‚îÄ Predicts intended movement

3. FEEDBACK
   ‚îú‚îÄ Cursor moves on screen
   ‚îú‚îÄ Or robot arm moves
   ‚îî‚îÄ User sees the result

4. ADAPTATION
   ‚îú‚îÄ Brain sees cursor position
   ‚îú‚îÄ Adjusts strategy if needed
   ‚îî‚îÄ Loop continues...

Time: 25-40ms per cycle ‚Üí 25-40 updates per second
```

### Why Closed-Loop Matters

**1. User Learning**

Users can **learn to control BCIs** through feedback:

```
Day 1: User tries to move cursor right
       ‚Üí Cursor moves somewhat randomly
       ‚Üí Brain doesn't know what patterns work
       
Day 7: User tries to move cursor right
       ‚Üí Cursor moves right more consistently
       ‚Üí Brain has learned which neural patterns work
       
Day 30: User moves cursor accurately
        ‚Üí Brain has optimized control strategy
        ‚Üí Feels natural, almost effortless
```

**Neural plasticity in action!** The brain rewires itself to work with the decoder.

**2. Co-Adaptation**

Both the **brain** and **decoder** adapt together:

```
Traditional approach:
1. Record data (open-loop)
2. Train decoder offline
3. Test decoder (closed-loop)
4. If bad, go back to step 1

Problem: Decoder never adapts to user's learning

Modern approach:
1. Start with initial decoder
2. User controls system (closed-loop)
3. Decoder updates continuously from user's actions
4. Brain adapts to decoder changes
5. Both improve together

Result: Faster learning, better performance!
```

**3. Error Correction**

The brain can **correct mistakes** in real-time:

```
Scenario: Decoder predicts "move left" but user wanted "move right"

Open-loop: User has no way to know or fix this
Closed-loop: 
  ‚Üí User sees cursor moving left (wrong!)
  ‚Üí Adjusts neural activity to correct
  ‚Üí After many corrections, decoder learns better
```

### Types of Feedback

**1. Visual Feedback (Most Common)**

```
Screen-based:
‚îú‚îÄ Cursor position
‚îú‚îÄ Target highlighting
‚îú‚îÄ Trajectory traces
‚îú‚îÄ Success/error indicators
‚îî‚îÄ Reward animations

Example: User sees cursor move as they think
```

**2. Haptic Feedback**

```
Physical sensations:
‚îú‚îÄ Vibration motors
‚îú‚îÄ Force feedback
‚îú‚îÄ Texture simulation
‚îî‚îÄ Resistance in robotic limbs

Example: Prosthetic hand vibrates when gripping object
```

**3. Auditory Feedback**

```
Sound-based:
‚îú‚îÄ Tones (pitch indicates speed/accuracy)
‚îú‚îÄ Clicks (confirm actions)
‚îú‚îÄ Music (modulated by performance)
‚îî‚îÄ Verbal feedback

Example: Higher pitch = moving faster
```

**4. Sensory Substitution**

```
Replacing lost senses:
‚îú‚îÄ Electrical stimulation
‚îú‚îÄ Intracortical microstimulation (ICMS)
‚îú‚îÄ Sensory neurons activated directly
‚îî‚îÄ Brain interprets as "feeling"

Example: Stimulating somatosensory cortex to create 
         sensation of touch in prosthetic hand
```

### Closed-Loop Decoding Strategies

**1. Supervised Learning with Update**

```python
# Initial training (open-loop data)
decoder.fit(neural_data, kinematics)

# Closed-loop operation
while using_bci:
    # Decode
    spikes = read_neural_activity()
    predicted_vel = decoder.predict(spikes)
    
    # Execute
    move_cursor(predicted_vel)
    
    # Get user's actual intention (e.g., from clicks)
    actual_vel = get_user_intention()
    
    # Update decoder
    decoder.partial_fit(spikes, actual_vel)
```

**2. Reinforcement Learning**

```python
# No labeled data needed!
# Decoder learns from rewards

while using_bci:
    spikes = read_neural_activity()
    action = decoder.predict(spikes)
    
    # Execute action
    new_state = execute_action(action)
    
    # Get reward (did it help reach the target?)
    reward = compute_reward(new_state, target)
    
    # Update decoder using reward signal
    decoder.update(spikes, action, reward)
```

**3. Kalman Filter (Adaptive)**

```
Classic closed-loop decoder:

State estimation:
‚îú‚îÄ Predicts future state from past observations
‚îú‚îÄ Updates prediction when new data arrives
‚îú‚îÄ Balances model predictions vs. measurements
‚îî‚îÄ Adapts to changing statistics

Perfect for smooth cursor control!
```

### Real Closed-Loop Examples

**1. BrainGate Cursor Control**

```
Setup:
- Utah array in motor cortex
- 96 electrodes recording
- Screen with cursor and targets
- Real-time decoding at 30Hz

User experience:
"I imagine moving my arm right
‚Üí I see the cursor move right
‚Üí I adjust my thought to move it more
‚Üí Cursor responds smoothly
‚Üí I can click on icons, browse web, play games"
```

**Performance:**
- Expert users: 90%+ accuracy
- Control feels natural after practice
- Can achieve 40-90 characters/minute typing

**2. Prosthetic Arm Control**

```
Setup:
- Neural recording from motor cortex
- Decoder predicts arm movement
- Robotic arm executes movement
- Force sensors provide feedback
- Sensation sent back to brain

User experience:
"I think about grasping a cup
‚Üí Robotic hand moves to cup
‚Üí Fingers close around it
‚Üí I feel pressure (via electrical stim)
‚Üí I adjust grip strength
‚Üí I lift the cup successfully"
```

**Result:** Near-natural control, can perform daily tasks

**3. Communication BCI**

```
Setup:
- EEG cap (non-invasive)
- User imagines left vs. right hand movement
- Decoder predicts which side
- Letter selector highlights letters
- User "selects" letter by imagining correct movement

Speed: 5-15 characters/minute
Accuracy: 80-95%
```

### Challenges in Closed-Loop BCIs

**1. Latency**

```
Total loop time = Neural recording + Processing + Feedback delivery

Too slow (>100ms):
- Feels laggy
- Hard to learn
- Frustrating

Optimal (<50ms):
- Feels responsive
- Brain can adapt quickly
- Natural control
```

**2. Non-Stationarity**

```
Problem: Neural activity changes over time

Causes:
‚îú‚îÄ Electrodes shift slightly
‚îú‚îÄ Immune response
‚îú‚îÄ User fatigue
‚îú‚îÄ Attention changes
‚îî‚îÄ Neural plasticity

Solution: Continuously adapting decoders
```

**3. Stability vs. Adaptation**

```
Tradeoff:

Too stable:
‚Üí Decoder doesn't adapt
‚Üí Performance degrades over time

Too adaptive:
‚Üí Decoder changes too quickly  
‚Üí User can't keep up
‚Üí Unstable control

Sweet spot: Slow, continuous adaptation
```

### PhantomLink as a Closed-Loop Testbed

**How to simulate closed-loop with PhantomLink:**

```python
async def simulated_closed_loop():
    """
    Use actual kinematics as feedback to test
    how your decoder would perform in closed-loop
    """
    
    async with websockets.connect(url) as ws:
        while True:
            # Get packet
            packet = await ws.recv_json()
            
            # Decode
            spikes = packet['spikes']['spike_counts']
            predicted_vel = decoder.predict([spikes])
            
            # Compare to actual (feedback)
            actual_vel = (packet['kinematics']['vx'], 
                         packet['kinematics']['vy'])
            
            # Calculate error
            error = np.linalg.norm(predicted_vel - actual_vel)
            
            # Simulate user adaptation
            if error > threshold:
                # In real BCI, brain would adjust
                # Here, we retrain decoder online
                decoder.partial_fit([spikes], [actual_vel])
            
            # Track performance over time
            performance_history.append(error)
```

**Benefits:**
- Test decoder adaptation strategies
- Understand learning curves
- Optimize update rates
- Validate stability

**Limitations:**
- No actual user learning
- Can't test novel control strategies
- Kinematics are pre-recorded

But still valuable for algorithm development!

### Future: Brain-to-Brain Interfaces

**Ultimate closed-loop:**

```
Person A's Brain ‚Üí Decoder ‚Üí Internet ‚Üí Encoder ‚Üí Person B's Brain

Example:
- Person A imagines "move right"
- Decoded as command
- Sent to Person B's brain
- Electrical stimulation
- Person B's hand moves right involuntarily

Currently in early research phase!
```

---

## How It All Fits Together

```
1. LOADING
   NWB File ‚Üí MC_MazeLoader ‚Üí Parse structure

2. ORGANIZATION  
   Raw data ‚Üí Organize by trials, targets, channels

3. STREAMING
   Request arrives ‚Üí PlaybackEngine ‚Üí Generate packet

4. DELIVERY
   Packet ‚Üí JSON ‚Üí WebSocket ‚Üí Your decoder

5. DECODING (your code!)
   Packet ‚Üí Feature extraction ‚Üí ML model ‚Üí Prediction
```

### Filtering Options

PhantomLink lets you filter the stream:

**By Target:**
```
ws://localhost:8000/stream?target_id=0
‚Üí Only get packets when reaching for Target 0
‚Üí Useful for training direction-specific decoders
```

**By Trial:**
```
ws://localhost:8000/stream?trial_id=42
‚Üí Only get packets from Trial 42
‚Üí Useful for analyzing specific movements
```

**All Data:**
```
ws://localhost:8000/stream
‚Üí Get everything in order
‚Üí Useful for full dataset analysis
```

---

## Real-World Applications

### 1. Assistive Technology

**For Paralyzed Individuals:**
- Control computer cursor with thoughts
- Type messages using on-screen keyboard
- Control smart home devices
- Operate robotic arms
- Drive wheelchairs

**PhantomLink helps by:**
- Letting developers build and test control algorithms
- Training AI models without needing live subjects
- Iterating quickly on decoder designs

### 2. Prosthetics

**Robotic Limbs:**
- Natural control of prosthetic arms/hands
- Fine motor control (grasping, pointing)
- Sensory feedback integration

**PhantomLink helps by:**
- Simulating different movement intentions
- Testing control algorithms for different gestures
- Validating real-time performance

### 3. Communication

**For People Who Can't Speak:**
- Direct brain-to-text
- Thought-to-speech synthesis
- High-speed communication

**Example:** A person with ALS can "type" at 90 characters per minute using only brain signals!

### 4. Research

**Neuroscience Studies:**
- Understanding how the brain encodes movement
- Studying neural plasticity
- Mapping brain function

**PhantomLink helps by:**
- Providing accessible research-grade data
- Enabling reproducible experiments
- Teaching students about neural coding

### 5. Gaming & VR

**Future Applications:**
- Control games with thoughts
- Enhanced VR immersion
- Brain-based authentication

### 6. Medicine

**Clinical Applications:**
- Seizure prediction
- Parkinson's treatment optimization
- Stroke rehabilitation monitoring

---

## Technical Deep Dive

### The Dataset: Neural Latents Benchmark

**Source:** [Neural Latents Benchmark](https://neurallatents.github.io/)  
**Dataset:** MC_Maze (Motor Cortex, Maze Task)

**Recording Details:**
- **Subject:** Non-human primate (monkey)
- **Implant:** Utah array (10√ó10 electrode grid)
- **Location:** Primary motor cortex (M1)
- **Task:** Center-out reaching with cursor
- **Recording Duration:** 293.7 seconds (~5 minutes)
- **Sampling Rate:** Behavior at 40Hz, neural spikes timestamped

**Data Format:**
- Stored in NWB (Neurodata Without Borders) format
- HDF5-based file structure
- Contains neural, behavioral, and trial metadata

### How PhantomLink Works

**Architecture:**
```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  NWB File   ‚îÇ (Raw data)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ MC_MazeLoader   ‚îÇ (Lazy loading, memory mapping)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
         ‚îÇ
         ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ PlaybackEngine   ‚îÇ (40Hz packet generation)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ FastAPI Server  ‚îÇ (HTTP + WebSocket endpoints)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
          ‚îÇ
          ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Your Decoder    ‚îÇ (Receives packets, makes predictions)
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Key Technologies:**
- **FastAPI:** Modern Python web framework
- **WebSockets:** Real-time bi-directional communication
- **PyNWB:** Reading neuroscience data files
- **NumPy:** Efficient numerical computations
- **Asyncio:** Concurrent request handling

### Packet Generation Process

**Step-by-step:**

1. **Client connects** to WebSocket endpoint
2. **PlaybackEngine** starts at index 0
3. **Every 25ms:**
   ```python
   # Calculate time window
   start_time = index * 0.025
   end_time = start_time + 0.025
   
   # Extract neural data
   spikes = loader.get_binned_spikes(start_time, end_time)
   
   # Extract behavioral data
   kinematics = loader.get_kinematics(start_time, end_time)
   
   # Get trial context
   trial_info = loader.get_trial_by_time(start_time)
   
   # Build packet
   packet = StreamPacket(
       timestamp=current_time,
       sequence_number=index,
       spikes=spikes,
       kinematics=kinematics,
       intention=trial_info,
       trial_id=trial_info.id
   )
   
   # Send over WebSocket
   await websocket.send_json(packet.dict())
   
   index += 1
   ```

4. **Repeat** until dataset ends or client disconnects

### Session Management

PhantomLink uses a **ChatGPT-style architecture:**

```
Client 1: /session/create
          ‚Üí Gets unique session ID: "neural-ace-42"
          ‚Üí Stream URL: ws://localhost:8000/stream/neural-ace-42

Client 2: /session/create  
          ‚Üí Gets unique session ID: "swift-neural-17"
          ‚Üí Stream URL: ws://localhost:8000/stream/swift-neural-17
```

**Benefits:**
- Multiple users can test simultaneously
- Each gets independent playback
- Shareable URLs for collaboration
- Isolated state per session

---

## Learning Resources

### Recommended Reading

**For Complete Beginners:**
1. "Brain-Computer Interfaces: Principles and Practice" - Wolpaw & Wolpaw
2. "Neuronal Dynamics" - Gerstner et al. (free online)
3. Khan Academy: Neuroscience basics

**For Developers:**
1. Neural Latents Benchmark documentation
2. PyNWB tutorials
3. FastAPI documentation
4. This repository's code!

### Online Courses

1. **Coursera:** Computational Neuroscience (University of Washington)
2. **edX:** Fundamentals of Neuroscience (Harvard)
3. **YouTube:** 3Blue1Brown's Neural Networks series

### Research Papers

**Classic Papers:**
1. Georgopoulos et al. (1982) - "Neuronal population coding of movement direction"
2. Hochberg et al. (2006) - "Neuronal ensemble control of prosthetic devices by a human with tetraplegia"

**Modern Reviews:**
1. Willett et al. (2021) - "High-performance brain-to-text communication via handwriting"
2. Neural Latents Benchmark paper

### Datasets & Tools

1. **Neural Latents Benchmark** - Multiple motor cortex datasets
2. **NWB Explorer** - Visualize NWB files
3. **CRCNS** - Collaborative Research in Computational Neuroscience datasets

### Communities

1. **Reddit:** r/neuroscience, r/neuroengineering
2. **Discord:** NeuroTech Berkeley, BrainAI
3. **Twitter:** #BCI, #neuroscience, #neurotech

### Hands-On Projects

**Start Here:**
1. Run PhantomLink locally
2. Open the [data_analysis.ipynb](data_analysis.ipynb) notebook
3. Build a simple linear decoder
4. Try filtering by target and compare accuracy

**Next Steps:**
1. Implement a Kalman filter decoder
2. Add real-time visualization
3. Build a cursor control demo
4. Train a neural network decoder

---

## Common Questions

### Q: Do I need neuroscience knowledge to use PhantomLink?

**A:** No! While it helps, PhantomLink is designed to be accessible. This guide covers everything you need to get started.

### Q: Can I use this data for my own projects?

**A:** Yes! The MC_Maze dataset is publicly available for research and educational purposes. Check the Neural Latents Benchmark license.

### Q: Is this data from humans?

**A:** No, it's from a non-human primate (monkey). Human BCI data is much more restricted due to privacy and ethics.

### Q: How accurate are current BCIs?

**A:** Modern research BCIs can achieve:
- **Cursor control:** 90%+ accuracy
- **Typing:** 90 characters/minute
- **Robotic arm control:** Smooth, natural movements

But there's still much room for improvement!

### Q: What programming languages do I need?

**A:** Python is the primary language. You should know:
- Basic Python syntax
- NumPy for arrays
- Basic async/await (for WebSockets)

### Q: How much data does PhantomLink stream?

**A:** About 50-100 KB per second, depending on the number of channels. Very manageable for modern networks.

### Q: Can I add my own datasets?

**A:** Not directly, but you can fork PhantomLink and adapt the DataLoader for your NWB files!

### Q: What machine learning models work best?

**A:** Common approaches:
- **Kalman Filter** (classic, simple, fast)
- **Linear Regression** (surprisingly effective baseline)
- **RNNs/LSTMs** (capture temporal dynamics)
- **Transformers** (state-of-the-art, but overkill for many tasks)

Start simple, then add complexity as needed!

---

## Next Steps

### 1. Get Started

```bash
# Clone the repository
git clone https://github.com/yelabb/PhantomLink.git
cd PhantomLink

# Install dependencies
pip install -r requirements.txt

# Run the server
python server.py

# Open the data analysis notebook
jupyter notebook data_analysis.ipynb
```

### 2. Explore the Data

- Run all cells in `data_analysis.ipynb`
- Look at the visualizations
- Understand the packet structure

### 3. Build Your First Decoder

Start with a simple project:
```python
# Pseudo-code for a basic decoder
async def simple_decoder():
    async with websockets.connect("ws://localhost:8000/stream") as ws:
        while True:
            packet = await ws.recv_json()
            
            # Extract features (simplified)
            spike_counts = packet['spikes']['spike_counts']
            avg_activity = np.mean(spike_counts)
            
            # Predict (simplified)
            if avg_activity > threshold:
                print("Moving!")
            else:
                print("Resting")
```

### 4. Join the Community

- Star this repository ‚≠ê
- Join neuroscience/BCI communities
- Share your projects
- Contribute improvements

---

## Glossary

**Action Potential** - Electrical signal that travels along a neuron (also called a "spike")

**BCI** - Brain-Computer Interface; direct connection between brain and computer

**Closed-Loop** - System where user receives feedback and can adapt their brain activity in real-time

**Decoder** - Algorithm that translates brain signals into intended actions

**Electrode** - Tiny sensor that records electrical activity in the brain

**Kinematics** - Description of motion (position, velocity, acceleration) without considering forces

**M1** - Primary motor cortex; brain region that controls voluntary movement

**Manifold** - Low-dimensional space that captures essential patterns of high-dimensional neural activity

**Neural Channel** - One recording site/electrode in a brain implant

**NWB** - Neurodata Without Borders; standard format for neuroscience data

**Open-Loop** - System where brain activity is recorded but user receives no feedback

**Packet** - Single snapshot of brain activity and behavior at one moment

**PCA** - Principal Component Analysis; technique for dimensionality reduction

**Spike** - Brief electrical signal from a neuron (action potential)

**Target** - Location the subject is trying to reach

**Trial** - One complete attempt at the task

**UMAP** - Uniform Manifold Approximation and Projection; technique for visualizing high-dimensional data

**Utah Array** - Grid of 100 electrodes implanted in the brain

**Velocity** - Speed and direction of movement (vx, vy components)

---

## Conclusion

PhantomLink makes BCI development accessible to everyone. Whether you're a:
- üéì **Student** learning about neuroscience
- üë®‚Äçüíª **Developer** building BCI applications
- üî¨ **Researcher** studying neural coding
- ü§î **Curious person** wanting to understand brains

...you now have the tools and knowledge to explore real brain data!

**Remember:** Every major technology started with people like you experimenting and learning. Today's educational project could become tomorrow's life-changing BCI application.

---

**Questions? Found a bug? Want to contribute?**

Open an issue on [GitHub](https://github.com/yelabb/PhantomLink) or check out the [main README](README.md)!

---

*Last updated: January 12, 2026*  
*Made with üß† and ‚ù§Ô∏è for the neuroscience community*
